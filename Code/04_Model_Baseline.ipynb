{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Baseline\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "dskjfnasdljfn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "#### External Libraries Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LassoCV, RidgeCV, ElasticNetCV, LinearRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read Cleaned Dataframes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from method 1\n",
    "df_pga1 = pd.read_csv('../Data/Sets/model_one.csv')\n",
    "# from method 2\n",
    "df_pga2 = pd.read_csv('../Data/Sets/model_two.csv')\n",
    "# from method 3\n",
    "df_pga3 = pd.read_csv('../Data/Sets/model_three.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "## Preparation for Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of features, remove irrelevant columns and strokes gained columns\n",
    "features = [\n",
    "    col for col in df_pga1.columns if col not in ['date', 'finish', \n",
    "                                                  'player', 'event', \n",
    "                                                  'sg:_off-the-tee',\n",
    "                                                  'sg:_approach-the-green',\n",
    "                                                  'sg:_around-the-green',\n",
    "                                                  'sg:_putting',\n",
    "                                                  'sg:_total']]\n",
    "\n",
    "X1 = df_pga1[features]\n",
    "y1 = df_pga1['sg:_total']\n",
    "\n",
    "# train, test split\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X1, y1, \n",
    "                                                        test_size = 0.3, random_state = 77)\n",
    "\n",
    "ss = StandardScaler()\n",
    "X_train1_sc = ss.fit_transform(X_train1)\n",
    "X_test1_sc = ss.transform(X_test1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of features, remove irrelevant columns and strokes gained columns\n",
    "features = [\n",
    "    col for col in df_pga2.columns if col not in ['date', 'finish', \n",
    "                                                  'player', 'event', \n",
    "                                                  'sg:_off-the-tee',\n",
    "                                                  'sg:_approach-the-green',\n",
    "                                                  'sg:_around-the-green',\n",
    "                                                  'sg:_putting',\n",
    "                                                  'sg:_total']]\n",
    "\n",
    "X2 = df_pga2[features]\n",
    "y2 = df_pga2['sg:_total']\n",
    "\n",
    "# train, test split\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2, \n",
    "                                                        test_size = 0.3, random_state = 77)\n",
    "\n",
    "ss = StandardScaler()\n",
    "X_train2_sc = ss.fit_transform(X_train2)\n",
    "X_test2_sc = ss.transform(X_test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of features, remove irrelevant columns and strokes gained columns\n",
    "features = [\n",
    "    col for col in df_pga3.columns if col not in ['date', 'finish', \n",
    "                                                  'player', 'event', \n",
    "                                                  'sg:_off-the-tee',\n",
    "                                                  'sg:_approach-the-green',\n",
    "                                                  'sg:_around-the-green',\n",
    "                                                  'sg:_putting',\n",
    "                                                  'sg:_total']]\n",
    "\n",
    "X3 = df_pga3[features]\n",
    "y3 = df_pga3['sg:_total']\n",
    "\n",
    "# train, test split\n",
    "X_train3, X_test3, y_train3, y_test3 = train_test_split(X3, y3, \n",
    "                                                        test_size = 0.3, random_state = 77)\n",
    "\n",
    "ss = StandardScaler()\n",
    "X_train3_sc = ss.fit_transform(X_train3)\n",
    "X_test3_sc = ss.transform(X_test3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "## Comparing Models\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 produces an average R-squared of 0.5894856748890387.\n",
      "Model 2 produces an average R-squared of 0.5675283561852548.\n",
      "Model 3 produces an average R-squared of 0.5672825566325466.\n"
     ]
    }
   ],
   "source": [
    "# instantiate basic linear regression model\n",
    "lr = LinearRegression()\n",
    "# instantiate a KFold to shuffle the data on every cross fold\n",
    "kf = KFold(n_splits = 10 , shuffle = True , random_state = 77)\n",
    "\n",
    "# fit a model for each of the three models\n",
    "lr1_cv = cross_val_score(lr, X_train1_sc, y_train1, cv=kf).mean()\n",
    "lr2_cv = cross_val_score(lr, X_train2_sc, y_train2, cv=kf).mean()\n",
    "lr3_cv = cross_val_score(lr, X_train3_sc, y_train3, cv=kf).mean()\n",
    "\n",
    "print(f'Model 1 produces an average R-squared of {lr1_cv}.')\n",
    "print(f'Model 2 produces an average R-squared of {lr2_cv}.')\n",
    "print(f'Model 3 produces an average R-squared of {lr3_cv}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model 1 produces the highest R-squared score on the training data. The model explains 58.89% of variability in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 produces an R-squared score of 0.60282 on unseen data.\n",
      "Model 2 produces an R-squared score of 0.55102 on unseen data.\n",
      "Model 3 produces an R-squared score of 0.56553 on unseen data.\n"
     ]
    }
   ],
   "source": [
    "# instantiate three models\n",
    "lr1 = LinearRegression()\n",
    "lr2 = LinearRegression()\n",
    "lr3 = LinearRegression()\n",
    "\n",
    "# fit the three sets of data to their own model\n",
    "lr1.fit(X_train1_sc, y_train1)\n",
    "lr2.fit(X_train2_sc, y_train2)\n",
    "lr3.fit(X_train3_sc, y_train3)\n",
    "\n",
    "print(f'Model 1 produces an R-squared score \\\n",
    "of {round(lr1.score(X_test1_sc, y_test1), 5)} on unseen data.')\n",
    "\n",
    "print(f'Model 2 produces an R-squared score \\\n",
    "of {round(lr2.score(X_test2_sc, y_test2), 5)} on unseen data.')\n",
    "\n",
    "print(f'Model 3 produces an R-squared score \\\n",
    "of {round(lr3.score(X_test3_sc, y_test3), 5)} on unseen data.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "The difference between the three models is the data. It is different based on the how the missing data was dealt with in notebook 02. When comparing the performance of each model, model 1 performs better by 0.3 - 0.4, although they are fairly similar. The similarity of scores demonstrates the lack of importance of the missing data. In this regression analysis, Model 1 will be used in further model tuning and interpretation. Remember that for Model 1, the missing data was simply ignored, no imputation took place. Because of this, Model 1 offers lends to the simplicity and interpretability of the modeling."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
