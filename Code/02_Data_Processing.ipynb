{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>Data Processing\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "Because of the large number of missing values that exists in the dataset, this notebook aims to handle that misssing data as effectively as possible. In order to do this, the problem is approached from three different angles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### External Libraries Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functions import to_inches, impute\n",
    "from os import listdir\n",
    "from fancyimpute import IterativeImputer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read Cleaned Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4648, 84)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pga = pd.read_csv('../Data/Sets/full_pga_data.csv')\n",
    "\n",
    "# check dimensions\n",
    "df_pga.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert date to datetime\n",
    "df_pga['date'] = pd.to_datetime(df_pga['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using a regex filter get a list of distance columns\n",
    "# each of these features is in the form of '(feet)'\\ (inches)\"'\n",
    "distance_columns = df_pga.filter(\n",
    "    regex = 'distance_from|approaches|proximity|longest_putts|approach_|average_'\n",
    ").columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This list of columns will be used in the future for converting features in the form:  ' 20'\\ 6\" '  to total inches as a float."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "## <u> Handle Missing Data <u/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the missing data is considered NMAR (not missing at random). This means that a null datapoint is missing for a reason. For example: if Dustin Johnson never attempted a putt from 9 feet for a given tournament, there will be a missing value for that metric. Handling missing data that is NMAR can be difficult, so in attempt to handle it most appropriately, three unique methods are used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: Ignore Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This method shrinks the dataset to 3616 observations and 60 features.\n"
     ]
    }
   ],
   "source": [
    "# grab all columns with fewer missing values than 5% of the data\n",
    "drop_rows = []\n",
    "for col in df_pga.columns:\n",
    "    if df_pga[col].isnull().sum() < 0.05 * df_pga.shape[0]:\n",
    "        drop_rows.append(col)\n",
    "        \n",
    "# drop rows with missing values in those columns\n",
    "df_pga1 = df_pga.dropna(subset = drop_rows)\n",
    "\n",
    "# create a dataframe of all columns without missing data\n",
    "# grab all columns without any null values\n",
    "notnull = []\n",
    "for col in df_pga1.columns:\n",
    "    if df_pga1[col].isnull().sum() == 0:\n",
    "        notnull.append(col)\n",
    "df_notnull = df_pga1[notnull]\n",
    "print(f'This method shrinks the dataset to {df_notnull.shape[0]} \\\n",
    "observations and {df_notnull.shape[1]} features.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Although this method shrinks the dataset, this smaller dataframe contains no missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert Datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert distance columns to total inches using to_inches function in functions.py file\n",
    "for col in df_notnull.columns:\n",
    "    if col in distance_columns:\n",
    "        df_notnull[col] = df_notnull[col].apply(lambda x: to_inches(x))\n",
    "        \n",
    "# finish position to integer\n",
    "df_notnull['finish'] = df_notnull['finish'].apply(lambda x: x.strip('T')).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save to csv for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_notnull.to_csv('../Data/Sets/model_one.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "### Method 2: Hot-Deck Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hot-Deck imputation is the process of filling null values with a randomly selected observed value that the corresponding player has recorded at some point in his career. For example: if Dustin Johnson has a missing value in the 'Putting from 9' feet column, a percentage that he recorded from some randomly chosen tournament will replace that missing value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This method shrinks the number of features to 69 but keeps the 4648 observations.\n"
     ]
    }
   ],
   "source": [
    "# get a list of columns with more than 500 missing values and drop them\n",
    "drop_cols = []\n",
    "for col in df_pga.columns:\n",
    "    if df_pga[col].isnull().sum() > 500:\n",
    "        drop_cols.append(col)\n",
    "df_pga2 = df_pga.drop(columns = drop_cols)\n",
    "print(f'This method shrinks the number of features to {df_pga2.shape[1]} \\\n",
    "but keeps the {df_pga2.shape[0]} observations.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- These columns are being removed because it is unreasonable to fill that many missing values with \"fake data\". Additionally, the columns in the 'drop_cols' list are features like approaches from the rough and approach shots from unordinary distances. They would not have a heavy influence on the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert Datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert distance columns to total inches\n",
    "for col in df_pga2.columns:\n",
    "    if col in distance_columns:\n",
    "        df_pga2[col] = df_pga2[col].apply(lambda x: to_inches(x))\n",
    "        \n",
    "# finish to integer\n",
    "df_pga2['finish'] = df_pga2['finish'].apply(lambda x: x.strip('T')).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get list of players again from directory\n",
    "players = listdir('../Data/')\n",
    "\n",
    "# remove unwanted files/folders\n",
    "players.remove('.DS_Store') \n",
    "players.remove('.ipynb_checkpoints')\n",
    "players.remove('Sets')\n",
    "len(players)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hot-Deck Imputation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through each column that contains missing values. Replace the missing \n",
    "# values with a random choice from each players distribution of that feature.\n",
    "for col in df_pga2.columns:\n",
    "    if df_pga2[col].isnull().sum() > 0:  \n",
    "        new_vals = []\n",
    "        for player in players:\n",
    "            filled = impute(df_pga2, col, player) # uses impute function from functions.py file\n",
    "            new_vals.extend(filled)\n",
    "        df_pga2[col] = new_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Although the missing values are being filled with actual recorded numbers, the data is still fake."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save to csv for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pga2.to_csv('../Data/Sets/model_two.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "### Method 3: Encoded Columns for Missingness and Regression Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 3 takes four steps:\n",
    "- Step 1: Drop columns with too many missing values (same as previous methods).\n",
    "- Step 2: Drop rows with missing values in columns that rarely have missing values (same as Method 2).\n",
    "- Step 3: Created binary encoded columns for columns that contain missing values:\n",
    "    - 1 means the value is missing.\n",
    "    - 0 means the value is not missing.\n",
    "- Step 4: Use IterativeImputer to fill missing values with a prediction calculated by a regression using observed values for each tournament."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns with overwhelming missing values\n",
    "drop_cols = []\n",
    "for col in df_pga.columns:\n",
    "    if df_pga[col].isnull().sum() > 500:\n",
    "        drop_cols.append(col)\n",
    "df_pga3 = df_pga.drop(columns = drop_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drops 110 rows.\n"
     ]
    }
   ],
   "source": [
    "# grab all columns with less than 25 missing values\n",
    "drop_rows = []\n",
    "for col in df_pga3.columns:\n",
    "    if df_pga3[col].isnull().sum() < 25:\n",
    "        drop_rows.append(col)\n",
    "        \n",
    "# drop rows with missing data in those columns\n",
    "df_pga3.dropna(subset = drop_rows, inplace = True)\n",
    "print('Drops 110 rows.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dropping these 110 rows significantly reduces the number of columns with missing data. This makes the encoding step much easier by reducing the number of columns to encode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert distance columns to total inches\n",
    "for col in df_pga3.columns:\n",
    "    if col in distance_columns:\n",
    "        df_pga3[col] = df_pga3[col].apply(lambda x: to_inches(x))\n",
    "        \n",
    "# finish to integer\n",
    "df_pga3['finish'] = df_pga3['finish'].apply(lambda x: x.strip('T')).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add 22 new encoded columns for remaining columns that contain missing values.\n"
     ]
    }
   ],
   "source": [
    "# create an encoded column containing a 1 \n",
    "# if the value is missing and a 0 if it is not\n",
    "encode_list = []\n",
    "for col in df_pga3.columns:\n",
    "    if 25 < df_pga3[col].isnull().sum() < 500:\n",
    "        encode_list.append(col)\n",
    "        \n",
    "for col in encode_list:\n",
    "    df_pga3[f'{col}_encoded'] = df_pga3[col].map(lambda x: 1 if np.isnan(x) else 0)\n",
    "print('Add 22 new encoded columns for remaining columns that contain missing values.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Creating these binary columns serves to add information about missing values to the model. If the value was missing, the coefficient on the encoded column will account for the missingness. If it is not missing, it does not affect the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4538, 91)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter for only numeric columns to prepare for imputation\n",
    "num_cols = [col for col in df_pga3.columns if col not in ['date', 'player', 'event']]\n",
    "df_pga3 = df_pga3[num_cols]\n",
    "\n",
    "# Use IterativeImputer to fill missing values. \n",
    "# This library fills the missing values with a prediction calculated\n",
    "# from a regression based on other observed values\n",
    "myimputer = IterativeImputer()\n",
    "df_pga3_transformed = myimputer.fit_transform(df_pga3)\n",
    "df_filled = pd.DataFrame(df_pga3_transformed)  \n",
    "\n",
    "# rename the columns\n",
    "df_filled.columns = df_pga3.columns\n",
    "\n",
    "# add the non-numeric columns\n",
    "df_filled[['date', 'player', 'event']] = df_pga[['date', 'player', 'event']]\n",
    "\n",
    "# check dimensions\n",
    "df_filled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The IterativeImputer regresses all the features on each other in order to make predictions for the missing values. This means that in some cases, it is using fake data to predict fake data. However, it may be a more accurate number compared to the imputation used in Method 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save to csv for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filled.to_csv('../Data/Sets/model_three.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
